# model arguments
model_name_or_path: /gaueko1/hizkuntza-ereduak/LLaMA/lm/huggingface/65B
use_fast_tokenizer: false
torch_dtype: float16

# dataset arguments
dataset_name: HiTZ/alpaca_mt
dataset_config_names:
  - en
  - pt
  - es
  - ca
  - eu
  - gl
  - at
validation_split_percentage: 4
block_size: 256

# checkpoint settings
output_dir: out/alpaca-lora-65b-en-pt-es-ca-eu-gl-at
overwrite_output_dir: true
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false
save_steps: 100
save_total_limit: 2

# evaluation
do_train: true
do_eval: true
evaluation_strategy: steps
logging_steps: 10
eval_steps: 100

# batch size: 2 batch size * 64 gradaccum * 2 GPUs = 128
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 32

# optimizer settings
optim: adamw_torch
learning_rate: 0.0003
weight_decay: 0
num_train_epochs: 1
lr_scheduler_type: cosine
warmup_ratio: 0.03

# lora settings
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules:
  - q_proj
  - v_proj

# reporting
report_to: wandb
run_name: alpaca-lora-65b-en-pt-es-ca-eu-gl-at

# hub settings
push_to_hub: true
resume_from_checkpoint: false
hub_model_id: HiTZ/alpaca-lora-65b-en-pt-es-ca-eu-gl-at
hub_private_repo: true

# performance
# bf16: true
fp16: true
torch_compile: false
ddp_find_unused_parameters: false
